# Description of phases:
# Phase1: just change partition types and make partitions 
# Phase2: create the md array on second disk, reboot
# Phase3: copy stuff from actual disk to md array, reboot from md array
# Phase4: destroy old disk by adding it to the array, final reboot


##### PHASE 1 #####
aptitude install mdadm

#change partition types
fdisk /dev/sda
#change partition type, t, to fd
#check bootable flag

sfdisk -d /dev/sda | sfdisk /dev/sdb
fdisk -l

#add md and raid1 to /etc/modules

#to avoid reboot at this phase, also modprobe them
modprobe md
modprobe raid1
cat /proc/mdstat

##### PHASE 2 #####


#note I had to 
mdadm --zero-superblock /dev/sdb 
mdadm --zero-superblock /dev/sdb1
mdadm --zero-superblock /dev/sdb2
#in case they were left over from previous tries during testing.



mdadm --create /dev/md2 --level=1 --raid-devices=2 missing /dev/sdb2
mkfs.ext3 /dev/md2


#testing stop and reassembly, check /proc/mdstat for status
mdadm --stop --scan
cat /proc/mdstat
mdadm -Ac partitions /dev/md2 -m dev
cat /proc/mdstat


cp /etc/mdadm/mdadm.conf /etc/mdadm/mdadm.conf-original
mdadm --examine --scan >> /etc/mdadm/mdadm.conf
#edit mdadm.conf, make sure array line is there

#other examples, not used
#DEVICES /dev/sdb2
#ARRAY /dev/md2	devices=missing,/dev/sdb2


#grub.conf
#do nothing, debian initrd should support raid1

##### REBOOT, check if your md array starts


##### PHASE 3 #####


mkdir /mnt/newroot
mount -t ext3 /dev/md2 /mnt/newroot/

#copying all stuff to the new drive
cp -aix / /mnt/newroot



#modify /mnt/newroot/etc/fstab
vim /mnt/newroot/etc/fstab

# /etc/fstab: static file system information.
#
# <file system> <mount point>   <type>  <options>       <dump>  <pass>
proc            /proc           proc    defaults        0       0
/dev/md2        /               ext3    errors=remount-ro 0       1
/dev/sdb1       none            swap    sw              0       0

##grub trick

#edit /boot/grub/menu.lst

default saved
#default 1
fallback 0
[new entry with /dev/md2 at position 1]
panic=10  #add to end of line with the md2 boot


#grub from boot 
grub> savedefault --default=1 --once

grub> quit

#or instead install regular in case you changed default
#grub> root (hd0,1)
#grub> setup (hd0)


##### REBOOT


##### PHASE 4 #####

#confirm that you rebooted from raid array
mount


##REINSTALL grub ON BOTH DISKS

#fix /boot/grub/menu.lst, change /dev/sda2 to /dev/md2
#:%s/sda2/md2/g

#don't forget line below in menu.lst. Don't remove # at beginning.

# kopt=root=/dev/md2 ro


grub> root (hd0,1)
 Filesystem type is ext2fs, partition type 0xfd
grub> setup (hd0)

grub> root (hd1,1)
 Filesystem type is ext2fs, partition type 0xfd
grub> setup (hd1)


###POINT OF NO RETURN
mdadm /dev/md2 -a /dev/sda2
watch -n 1 cat /proc/mdstat


#fix /etc/mdadm/mdadm.conf, (even though it is not used?)
#DEVICE /dev/sda2 /dev/sdb2
#ARRAY /dev/md2 devices=/dev/sda2,/dev/sdb2
mdadm --examine --scan >> /etc/mdadm/mdadm.conf


#do final checks, check these files
# /boot/grub/menu.lst
# /etc/fstab
# /etc/mdadm/mdadm.conf    #i think this is not used.

##### FINAL REBOOT, good luck


###############################################################################

#optional, changing the swaps to raid1 as well
swapoff /dev/sda1
#change their types to fd with fdisk
fdisk /dev/sda
fdisk /dev/sdb
mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sda1 /dev/sdb1
mkswap /dev/md1
swapon /dev/md1

#add it to /etc/fstab

###############################################################################

#tests:

#try failing drive sda on purpose
#check if installing new kernels still updates the second drive bootblock 

title		Debian GNU/Linux, kernel 2.6.26-2-vserver-amd64
root		(hd0,1)
kernel		/boot/vmlinuz-2.6.26-2-vserver-amd64 root=/dev/md2 ro 
initrd		/boot/initrd.img-2.6.26-2-vserver-amd64

title		Debian GNU/Linux, kernel 2.6.26-2-vserver-amd64 (single-user mode)
root		(hd0,1)
kernel		/boot/vmlinuz-2.6.26-2-vserver-amd64 root=/dev/md2 ro single
initrd		/boot/initrd.img-2.6.26-2-vserver-amd64

title		Debian GNU/Linux, kernel 2.6.26-2-amd64
root		(hd0,1)
kernel		/boot/vmlinuz-2.6.26-2-amd64 root=/dev/md2 ro 
initrd		/boot/initrd.img-2.6.26-2-amd64

title		Debian GNU/Linux, kernel 2.6.26-2-amd64 (single-user mode)
root		(hd0,1)
kernel		/boot/vmlinuz-2.6.26-2-amd64 root=/dev/md2 ro single
initrd		/boot/initrd.img-2.6.26-2-amd64

title		Debian GNU/Linux, kernel 2.6.18-6-amd64
root		(hd0,1)
kernel		/boot/vmlinuz-2.6.18-6-amd64 root=/dev/md2 ro 
initrd		/boot/initrd.img-2.6.18-6-amd64

title		Debian GNU/Linux, kernel 2.6.18-6-amd64 (single-user mode)
root		(hd0,1)
kernel		/boot/vmlinuz-2.6.18-6-amd64 root=/dev/md2 ro single
initrd		/boot/initrd.img-2.6.18-6-amd64










########### recovery ###########
http://www.freddenny.com/UNIX/linuxRAID.html

cat /proc/mdstat
fdisk -l
fdisk /dev/sdb
 Disk /dev/sdb: 255 heads, 63 sectors, 4462 cylinders
 Units = cylinders of 16065 * 512 bytes
 Device Boot Start End Blocks Id System
 /dev/sdb1 1 261 2096451 6 FAT16

 Command (m for help): d
 Partition number (1-4): 1

 Command (m for help): n
 Command action
 e extended
 p primary partition (1-4)
 p
 Partition number (1-4): 1
 First cylinder (1-4462, default 1):
 Using default value 1
 Last cylinder or +size or +sizeM or +sizeK (1-4462, default 4462):
 Using default value 4462

 Command (m for help): t
 Partition number (1-4): 1
 Hex code (type L to list codes): fd
 Changed system type of partition 1 to fd (Linux raid autodetect)

 Command (m for help): w
 The partition table has been altered!
 Calling ioctl() to re-read partition table.
 Syncing disks.

cat /proc/mdstat
raidhotadd /dev/md0 /dev/sdb1
cat /proc/mdstat

#############
http://conshell.net/wiki/index.php/Software_RAID_Recovery_on_Linux
# df -h
Filesystem            Size  Used Avail Use% Mounted on
/dev/md1              487M  320M  142M  70% /
/dev/md0               99M   16M   79M  17% /boot
tmpfs                 2.0G     0  2.0G   0% /dev/shm
/dev/md2               15G  8.8G  4.8G  65% /home
/dev/md4              7.8G  2.0G  5.4G  27% /usr
/dev/md5              259G   20G  227G   8% /var
/dev/sdc1             2.8T  1.9T  847G  70% /mnt/backup

#cat /proc/mdstat
Personalities : [raid1] [raid6] [raid5] [raid4] 
md0 : active raid1 sda1[0]
      104320 blocks [2/1] [U_]
      
md2 : active raid1 sda2[0]
      15358016 blocks [2/1] [U_]
      
md3 : active raid1 sdb3[1] sda3[0]
      8393856 blocks [2/2] [UU]
      
md4 : active raid1 sda5[0]
      8393856 blocks [2/1] [U_]
      
md5 : active raid1 sda7[0]
      279803968 blocks [2/1] [U_]
      
md1 : active raid1 sda6[0]
      513984 blocks [2/1] [U_]
      
unused devices: <none>

#grep md: /var/log/dmesg
md:  adding sdb1 ...
md: md0 already running, cannot run sdb1
md: export_rdev(sdb1)
md: ... autorun DONE.

#cat /etc/mdadm.conf
DEVICE partitions
MAILADDR root
ARRAY /dev/md1 super-minor=1
ARRAY /dev/md0 super-minor=0
ARRAY /dev/md2 super-minor=2
ARRAY /dev/md4 super-minor=4
ARRAY /dev/md5 super-minor=5
ARRAY /dev/md3 super-minor=3

#mdadm --detail /dev/md0
/dev/md0:
        Version : 00.90.03
  Creation Time : Mon Feb 19 07:54:06 2007
     Raid Level : raid1
     Array Size : 104320 (101.89 MiB 106.82 MB)
    Device Size : 104320 (101.89 MiB 106.82 MB)
   Raid Devices : 2
  Total Devices : 1
Preferred Minor : 0
    Persistence : Superblock is persistent

    Update Time : Wed Feb  6 17:09:50 2008
          State : clean, degraded
 Active Devices : 1
Working Devices : 1
 Failed Devices : 0
  Spare Devices : 0

           UUID : 9f809745:8bf08609:14ed382c:d0ecabc6
         Events : 0.7840

    Number   Major   Minor   RaidDevice State
       0       8        1        0      active sync   /dev/sda1
       1       0        0        1      removed

#mdadm --manage /dev/md5 --re-add /dev/sdb7
mdadm: re-added /dev/sdb7

#mdadm --detail /dev/md5
/dev/md5:
        Version : 00.90.03
  Creation Time : Mon Feb 19 08:43:35 2007
     Raid Level : raid1
     Array Size : 279803968 (266.84 GiB 286.52 GB)
    Device Size : 279803968 (266.84 GiB 286.52 GB)
   Raid Devices : 2
  Total Devices : 2
Preferred Minor : 5
    Persistence : Superblock is persistent

    Update Time : Wed Feb 27 08:30:55 2008
          State : clean, degraded, recovering
 Active Devices : 1
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 1

 Rebuild Status : 0% complete

           UUID : ad6bc290:5fe289ce:8c051c7c:5f972730
         Events : 0.38424642

    Number   Major   Minor   RaidDevice State
       0       8        7        0      active sync   /dev/sda7
       2       8       23        1      spare rebuilding   /dev/sdb7

#cat /proc/mdstat 
...
md5 : active raid1 sdb7[2] sda7[0]
      279803968 blocks [2/1] [U_]
      [>....................]  recovery =  2.1% (6104128/279803968) finish=94.1min speed=48467K/sec
...      

Now do the same for the other degraded arrays.

#mdadm --manage /dev/md0 --re-add /dev/sdb1
#mdadm --manage /dev/md1 --re-add /dev/sdb6
#mdadm --manage /dev/md2 --re-add /dev/sdb2
#mdadm --manage /dev/md4 --re-add /dev/sdb5

A very handy command for displaying the progress of the rebuilds is:

#watch -d -n 10 cat /proc/mdstat















########## Recovery from a multiple disk failure  ############
http://unthought.net/Software-RAID.HOWTO/Software-RAID.HOWTO-8.html#ss8.1
The scenario is: p

    * A controller dies and takes two disks offline at the same time,
    * All disks on one scsi bus can no longer be reached if a disk dies,
    * A cable comes loose...

In short: quite often you get a temporary failure of several disks at once; afterwards the RAID superblocks are out of sync and you can no longer init your RAID array.

One thing left: rewrite the RAID superblocks by mkraid --force

To get this to work, you'll need to have an up to date /etc/raidtab - if it doesn't EXACTLY match devices and ordering of the original disks this will not work as expected, but will most likely completely obliterate whatever data you used to have on your disks.

Look at the sylog produced by trying to start the array, you'll see the event count for each superblock; usually it's best to leave out the disk with the lowest event count, i.e the oldest one.

If you mkraid without failed-disk, the recovery thread will kick in immediately and start rebuilding the parity blocks - not necessarily what you want at that moment.

With failed-disk you can specify exactly which disks you want to be active and perhaps try different combinations for best results. BTW, only mount the filesystem read-only while trying this out... This has been successfully used by at least two guys I've been in contact with. 
